@article{gorbunov2021secure,
  title={Secure Distributed Training at Scale},
  author={Gorbunov, Eduard and Borzunov, Alexander and Diskin, Michael and Ryabinin, Max},
  journal={arXiv preprint arXiv:2106.11257},
  year={2021}
}

@inproceedings{castro1999practical,
  title={Practical byzantine fault tolerance},
  author={Castro, Miguel and Liskov, Barbara and others},
  booktitle={OSDI},
  volume={99},
  number={1999},
  pages={173--186},
  year={1999}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@inproceedings{karimireddy2021learning,
  title={Learning from history for byzantine robust optimization},
  author={Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={5311--5319},
  year={2021},
  organization={PMLR}
}

@article{baruch2019little,
  title={A little is enough: Circumventing defenses for distributed learning},
  author={Baruch, Moran and Baruch, Gilad and Goldberg, Yoav},
  journal={arXiv preprint arXiv:1902.06156},
  year={2019}
}

@inproceedings{xie2020fall,
  title={Fall of empires: Breaking Byzantine-tolerant SGD by inner product manipulation},
  author={Xie, Cong and Koyejo, Oluwasanmi and Gupta, Indranil},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={261--270},
  year={2020},
  organization={PMLR}
}

@article{li2020pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  journal={arXiv preprint arXiv:2104.04473},
  year={2021}
}

@inproceedings{blanchard2017machine,
  title={Machine learning with adversaries: Byzantine tolerant gradient descent},
  author={Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={118--128},
  year={2017}
}

@article{sergeev2018horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

@misc{sagemaker_aws_docs,
title={Introduction to {SageMaker}'s Distributed Data Parallel Library},
url={https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html},
note={retrieved on 21 Dec 2021}, publisher={Amazon}, journal={Amazon SageMaker Developer Guide}, author={Amazon}, month={May}, year={2021}}

@misc{tensorflow_mirrored_doc,
title={{Distributed training}},
url={https://www.tensorflow.org/api\_docs/python/tf/distribute/MirroredStrategy},
note={retrieved on 23 Dec 2021}, journal={{TensorFlow}}, publisher={{TensorFlow}}, author={{TensorFlow}}, year={2021}}

@article{xie2018generalized,
  title={Generalized byzantine-tolerant sgd},
  author={Xie, Cong and Koyejo, Oluwasanmi and Gupta, Indranil},
  journal={arXiv preprint arXiv:1802.10116},
  year={2018}
}
@inproceedings{yin2018byzantine,
  title={Byzantine-robust distributed learning: Towards optimal statistical rates},
  author={Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={International Conference on Machine Learning},
  pages={5650--5659},
  year={2018},
  organization={PMLR}
}

@article{chen2017distributed,
  title={Distributed statistical machine learning in adversarial settings: Byzantine gradient descent},
  author={Chen, Yudong and Su, Lili and Xu, Jiaming},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={1},
  number={2},
  pages={1--25},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{prakash2020mitigating,
  title={Mitigating byzantine attacks in federated learning},
  author={Prakash, Saurav and Avestimehr, Amir Salman},
  journal={arXiv preprint arXiv:2010.07541},
  year={2020}
}
@article{el2018hidden,
  title={The Hidden Vulnerability of Distributed Learning in Byzantium},
  author={El El Mhamdi, Mahdi and Guerraoui, Rachid and Rouault, S{\'e}bastien},
  journal={arXiv e-prints},
  pages={arXiv--1802},
  year={2018}
}

@article{yang2019byrdie,
  title={ByRDiE: Byzantine-resilient distributed coordinate descent for decentralized learning},
  author={Yang, Zhixiong and Bajwa, Waheed U},
  journal={IEEE Transactions on Signal and Information Processing over Networks},
  volume={5},
  number={4},
  pages={611--627},
  year={2019},
  publisher={IEEE}
}

@inproceedings{gupta2021byzantine,
  title={Byzantine Fault-Tolerance in Decentralized Optimization under 2f-Redundancy},
  author={Gupta, Nirupam and Doan, Thinh T and Vaidya, Nitin H},
  booktitle={2021 American Control Conference (ACC)},
  pages={3632--3637},
  year={2021},
  organization={IEEE}
}

@article{patarasuk2009bandwidth,
  title={Bandwidth optimal all-reduce algorithms for clusters of workstations},
  author={Patarasuk, Pitch and Yuan, Xin},
  journal={Journal of Parallel and Distributed Computing},
  volume={69},
  number={2},
  pages={117--124},
  year={2009},
  publisher={Elsevier}
}


@misc{mnist_database,
title={The {MNIST} Database of handwritten digits},
url={http://yann.lecun.com/exdb/mnist/},
note={retrieved on 23 Dec 2021}, publisher={yann.lecun.com}, journal={yann.lecun.com}, author={LeCun, Yann and Cortes, Corinna and Burges, Christopher J.C.}, month={May}, year={1998}}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@incollection{li2017efficient,
  title={An efficient task-based all-reduce for machine learning applications},
  author={Li, Zhenyu and Davis, James and Jarvis, Stephen},
  booktitle={Proceedings of the Machine Learning on HPC Environments},
  pages={1--8},
  year={2017}
}